{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated BPTT\n",
    "\n",
    "When the sequences are very long (thousands of points), the network training can be very slow and the memory requirements increase. The truncated BPTT is an alternative similar to mini-batch training in Dense Networks, even though in RNN the batch parameter can also be used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2020.deeplearning/master/init.py\n",
    "from init import init; init(force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "width": 1200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='local/imgs/rnn_tbptt_2.png', width=1200)\n",
    "#![alt text](local/imgs/rnn_tbptt_2.png \"Truncated BPTT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TBPTT can be implemented by setting up the data appropriately. Let's remember that for recurrent neural networks, data must have the format **[n_samples,n_times,n_features]**, so if you want to use Truncated BPTT you just have to split the sequences into more **n_samples** of less **n_times**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BUT**, **Is it possible that the LSTM may find dependencies between the sequences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it’s not possible unless you go for the stateful LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the use of Truncated BPTT requires to set up the **Stateful** mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Extracted from: http://philipperemy.github.io/keras-stateful-lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also a problem of classifying sequences. The data matrix $X$ is made exclusively of zeros except in the first column where exactly half of the values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "N_train = 1000\n",
    "X_train = np.zeros((N_train,20))\n",
    "one_indexes = choice(a=N_train, size=int(N_train / 2), replace=False)\n",
    "X_train[one_indexes, 0] = 1  # very long term memory.\n",
    "#--------------------------------\n",
    "N_test = 200\n",
    "X_test = np.zeros((N_test,20))\n",
    "one_indexes = choice(a=N_test, size=int(N_test / 2), replace=False)\n",
    "X_test[one_indexes, 0] = 1  # very long term memory.\n",
    "print(X_train[:10,:5])\n",
    "print(X_test[:10,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(x_train, y_train, window_length, increment):\n",
    "    windows = []\n",
    "    windows_y = []\n",
    "    for i, sequence in enumerate(x_train):\n",
    "        len_seq = len(sequence)\n",
    "        for window_start in range(0, len_seq - window_length + 1, increment):\n",
    "            window_end = window_start + window_length\n",
    "            window = sequence[window_start:window_end]\n",
    "            windows.append(window)\n",
    "            windows_y.append(y_train[i])\n",
    "    return np.array(windows), np.array(windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10, 1)\n",
      "(2000, 1)\n",
      "(400, 10, 1)\n",
      "(400, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split the sequences into two sequences of length 10\n",
    "window_length = 10\n",
    "\n",
    "x_train, y_train = prepare_sequences(X_train, X_train[:,0], window_length,window_length)\n",
    "x_test, y_test = prepare_sequences(X_test, X_test[:,0], window_length,window_length)\n",
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every sequence was split into 2 subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:4,:])\n",
    "print(y_train[:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a regular LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the original sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building STATELESS model...\n",
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6455 - accuracy: 0.5920 - val_loss: 0.3801 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 439us/sample - loss: 0.2234 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1s 761us/sample - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1s 810us/sample - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1s 544us/sample - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('Building STATELESS model...')\n",
    "max_len = 10\n",
    "batch_size = 11\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(20, 1), return_sequences=False, stateful=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train.reshape(1000,20,1), X_train[:,0].reshape(1000,1), batch_size=batch_size, epochs=5,\n",
    "          validation_data=(X_test.reshape(200,20,1), X_test[:,0].reshape(200,1)), shuffle=False)\n",
    "score, acc = model.evaluate(X_test.reshape(200,20,1),X_test[:,0].reshape(200,1), batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the splitted sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building STATELESS model...\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 4s 2ms/sample - loss: 0.5201 - accuracy: 0.7270 - val_loss: 0.4791 - val_accuracy: 0.7500\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4812 - accuracy: 0.7500 - val_loss: 0.4785 - val_accuracy: 0.7500\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 4s 2ms/sample - loss: 0.4804 - accuracy: 0.7500 - val_loss: 0.4782 - val_accuracy: 0.7500\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 5s 2ms/sample - loss: 0.4800 - accuracy: 0.7500 - val_loss: 0.4781 - val_accuracy: 0.7500\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4797 - accuracy: 0.7500 - val_loss: 0.4780 - val_accuracy: 0.7500\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 7s 4ms/sample - loss: 0.4795 - accuracy: 0.7500 - val_loss: 0.4779 - val_accuracy: 0.7500\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4794 - accuracy: 0.7500 - val_loss: 0.4778 - val_accuracy: 0.7500\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4793 - accuracy: 0.7500 - val_loss: 0.4777 - val_accuracy: 0.7500\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 6s 3ms/sample - loss: 0.4792 - accuracy: 0.7500 - val_loss: 0.4777 - val_accuracy: 0.7500\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 5s 2ms/sample - loss: 0.4791 - accuracy: 0.7500 - val_loss: 0.4777 - val_accuracy: 0.7500\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 5s 3ms/sample - loss: 0.4790 - accuracy: 0.7500 - val_loss: 0.4776 - val_accuracy: 0.7500\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 4s 2ms/sample - loss: 0.4789 - accuracy: 0.7500 - val_loss: 0.4776 - val_accuracy: 0.7500\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 4s 2ms/sample - loss: 0.4788 - accuracy: 0.7500 - val_loss: 0.4776 - val_accuracy: 0.7500\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 4s 2ms/sample - loss: 0.4788 - accuracy: 0.7500 - val_loss: 0.4775 - val_accuracy: 0.7500\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 3s 2ms/sample - loss: 0.4787 - accuracy: 0.7500 - val_loss: 0.4775 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "print('Building STATELESS model...')\n",
    "max_len = 10\n",
    "batch_size = 2\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(max_len, 1), return_sequences=False, stateful=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=15,\n",
    "          validation_data=(x_test, y_test), shuffle=False)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences composed of 0s are correctly classified. The subsequences starting with 1 are correctly classified, but the sebsequences of class 1 starting with 0, are wrong classified. Those are the 25% of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long range memory required to classify the sequences correctly has been lost because of the sequences' partition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATEFUL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build STATEFUL model...\n"
     ]
    }
   ],
   "source": [
    "print('Build STATEFUL model...')\n",
    "max_len = 10\n",
    "n_partitions = 2\n",
    "batch = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, batch_input_shape=(batch, max_len, 1), return_sequences=False, stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "accuracy training = 0.9605000019073486\n",
      "loss training = 0.08138144016265869\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 0.0014561639400199056\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 0.0006937840953469276\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 0.0003019043360836804\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 0.0001738684659358114\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 9.1511283244472e-05\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 5.5697779316687956e-05\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 3.103803828707896e-05\n",
      "___________________________________\n",
      "accuracy training = 1.0\n",
      "loss training = 1.9282120774732903e-05\n",
      "___________________________________\n",
      "accuracy testing = 1.0\n",
      "loss testing = 1.0981940249621402e-05\n",
      "___________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "for epoch in range(5):\n",
    "    mean_tr_acc = []\n",
    "    mean_tr_loss = []\n",
    "    for i in range(0,x_train.shape[0],n_partitions):\n",
    "        #print(i)\n",
    "        for j in range(n_partitions):\n",
    "            #print(j)\n",
    "            tr_loss, tr_acc = model.train_on_batch(x_train[i+j,:,:].reshape(1,max_len,1), y_train[i+j,:].reshape(1,1))\n",
    "            mean_tr_acc.append(tr_acc)\n",
    "            mean_tr_loss.append(tr_loss)\n",
    "        model.reset_states()    \n",
    "    print('accuracy training = {}'.format(np.mean(mean_tr_acc)))\n",
    "    print('loss training = {}'.format(np.mean(mean_tr_loss)))\n",
    "    print('___________________________________')\n",
    "\n",
    "    mean_te_acc = []\n",
    "    mean_te_loss = []\n",
    "    for i in range(0,x_test.shape[0],n_partitions):\n",
    "        for j in range(n_partitions):\n",
    "            te_loss, te_acc = model.test_on_batch(x_test[i+j,:,:].reshape(1,max_len,1), y_test[i+j,:].reshape(1,1))\n",
    "            mean_te_acc.append(te_acc)\n",
    "            mean_te_loss.append(te_loss)\n",
    "        model.reset_states()\n",
    "\n",
    "    print('accuracy testing = {}'.format(np.mean(mean_te_acc)))\n",
    "    print('loss testing = {}'.format(np.mean(mean_te_loss)))\n",
    "    print('___________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code was a bit more difficult to write because we have to manually call **model.reset_states()** at each new sequence processed. Another method to do that is to write a callback that reset the states at each sequence like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 11s 5ms/sample - loss: 0.0740 - accuracy: 0.9625\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 10s 5ms/sample - loss: 4.8555e-04 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 12s 6ms/sample - loss: 1.2338e-04 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 10s 5ms/sample - loss: 3.9900e-05 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 8s 4ms/sample - loss: 1.3916e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82dc08b850>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "n_partitions = 2\n",
    "class ResetStatesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if self.counter % n_partitions == 0:\n",
    "            self.model.reset_states()\n",
    "        self.counter += 1\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(LSTM(10, batch_input_shape=(batch, max_len, 1), return_sequences=False, stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, callbacks=[ResetStatesCallback()], batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset for validation have a different batchsize, the best way to solve it, is to create a new model with the new batchsize and transfer to it the weights of the trained model.\n",
    "\n",
    "***Example**: The following code does not have relation with the previous examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network\n",
    "n_batch = 3\n",
    "n_epoch = 1000\n",
    "n_neurons = 10\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# fit network\n",
    "for i in range(n_epoch):\n",
    "    model.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "    model.reset_states()\n",
    "# re-define the batch size\n",
    "n_batch = 1\n",
    "# re-define model\n",
    "new_model = Sequential()\n",
    "new_model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "new_model.add(Dense(1))\n",
    "# copy weights\n",
    "old_weights = model.get_weights()\n",
    "new_model.set_weights(old_weights)\n",
    "# compile model\n",
    "new_model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
